# -*- coding: utf-8 -*-
"""Loreal Dataset Preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11bNMBRXnyRCxhH5Onxots27ItVNzLmxi
"""

import pandas as pd

# -----------------------------
# CONFIGURATION
# -----------------------------
# File paths (change these to your actual file names)
comments_files = [
    "comments1.csv",
    "comments2.csv",
    "comments3.csv",
    "comments4.csv",
    "comments5.csv"
]
videos_file = "videos.csv"

# Output file
merged_output_file = "merged_comments_videos.csv"

# -----------------------------
# STEP 1: Load all comments datasets in chunks
# -----------------------------
comments_list = []

for file in comments_files:
    print(f"Loading {file}...")

    # Read in chunks of 100,000 rows
    chunk_iter = pd.read_csv(file, chunksize=100000, low_memory=False)

    # Process each chunk
    for chunk in chunk_iter:
        # Keep only useful columns (you can add/remove based on your AI pipeline)
        if set(["commentId", "videoId", "textDisplay", "likeCount"]).issubset(chunk.columns):
            chunk = chunk[["commentId", "videoId", "textDisplay", "likeCount"]]
        else:
            print(f"Some columns not found in {file}, keeping all for now.")

        comments_list.append(chunk)

# Combine all comments into one big dataframe
all_comments = pd.concat(comments_list, ignore_index=True)
print(f"Total comments loaded: {len(all_comments)}")

# -----------------------------
# STEP 2: Load videos dataset
# -----------------------------
print("Loading videos dataset...")
videos = pd.read_csv(videos_file, low_memory=False)

# Keep only useful columns (you can adjust this)
if set(["videoId", "title", "categoryId"]).issubset(videos.columns):
    videos = videos[["videoId", "title", "categoryId"]]

print(f"Total videos loaded: {len(videos)}")

# -----------------------------
# STEP 3: Merge comments with videos on videoId
# -----------------------------
print("Merging comments with videos...")
merged_df = all_comments.merge(videos, on="videoId", how="left")

print(f"Final merged dataset shape: {merged_df.shape}")

# -----------------------------
# STEP 4: Save merged dataset
# -----------------------------
merged_df.to_csv(merged_output_file, index=False)
print(f"Merged dataset saved as {merged_output_file}")

import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.sentiment import SentimentIntensityAnalyzer

# -----------------------------
# STEP 0: Setup
# -----------------------------
nltk.download("stopwords")
nltk.download("wordnet")
nltk.download("vader_lexicon")

stop_words = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()
sia = SentimentIntensityAnalyzer()

# -----------------------------
# STEP 1: Load merged dataset
# -----------------------------
file_path = "merged_comments_videos.csv"
df = pd.read_csv(file_path, low_memory=False)

print("Original dataset shape:", df.shape)

# Keep only useful columns based on your file
useful_cols = ["commentId", "videoId", "textOriginal", "likeCount_x", "title", "categoryId"]
df = df[[col for col in useful_cols if col in df.columns]]

# Drop rows with no comments
df = df.dropna(subset=["textOriginal"])
df["likeCount_x"] = df["likeCount_x"].fillna(0)

# -----------------------------
# STEP 2: Text Cleaning
# -----------------------------
def clean_text(text):
    text = re.sub(r"<.*?>", " ", str(text))        # remove HTML tags
    text = re.sub(r"http\S+|www\S+", " ", text)    # remove URLs
    text = re.sub(r"[^a-zA-Z\s]", " ", text)       # remove emojis & symbols
    text = text.lower()                            # lowercase
    words = [word for word in text.split() if word not in stop_words]
    words = [lemmatizer.lemmatize(word) for word in words]
    return " ".join(words)

print("Cleaning comments...")
df["cleaned_text"] = df["textOriginal"].apply(clean_text)

# -----------------------------
# STEP 3: Sentiment Analysis
# -----------------------------
def get_sentiment(text):
    scores = sia.polarity_scores(text)
    compound = scores["compound"]
    if compound >= 0.05:
        return "Positive"
    elif compound <= -0.05:
        return "Negative"
    else:
        return "Neutral"

print("Running sentiment analysis...")
df["sentiment"] = df["cleaned_text"].apply(get_sentiment)

# -----------------------------
# STEP 4: Spam Detection
# -----------------------------
spam_keywords = ["http", "www", "buy now", "subscribe", "follow me", "check out", "promo", "discount"]

def is_spam(text):
    # Rule-based detection
    if any(keyword in text.lower() for keyword in spam_keywords):
        return "Spam"
    if len(text.split()) < 2:   # too short
        return "Spam"
    return "Not Spam"

print("Detecting spam...")
df["spam_flag"] = df["textOriginal"].apply(is_spam)

# -----------------------------
# STEP 5: Categorization
# -----------------------------
categories = {
    "skincare": ["skin", "moisturizer", "serum", "cream", "lotion", "cleanser", "toner", "mask"],
    "fragrance": ["perfume", "cologne", "fragrance", "scent", "eau de", "smell"],
    "makeup": ["lipstick", "foundation", "mascara", "eyeshadow", "blush", "concealer", "makeup"]
}

def categorize(text):
    text = text.lower()
    for cat, keywords in categories.items():
        if any(word in text for word in keywords):
            return cat
    return "other"

print("Categorizing comments...")
df["category"] = df["cleaned_text"].apply(categorize)

# -----------------------------
# STEP 6: Save Final Preprocessed Dataset
# -----------------------------
output_file = "final_preprocessed_comments.csv"
df.to_csv(output_file, index=False)

print("Full preprocessing complete!")
print("Final dataset shape:", df.shape)
print(f"Saved dataset as {output_file}")

import pandas as pd

df = pd.read_csv("final_preprocessed_comments.csv")
sample_df = df.sample(n=20000, random_state=42)   # pick 20k rows
sample_df.to_csv("sample_comments.csv", index=False)